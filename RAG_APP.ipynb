{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1-5pT7XzD0YVTJLcDMOyHmwNXu_NDrFwB",
      "authorship_tag": "ABX9TyP5GevRcj0DpUyMxRcl3A4r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dharshan4038/PDF_Q-A_USING_RAG/blob/main/RAG_APP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gOOTZbfV3WPY"
      },
      "outputs": [],
      "source": [
        "# %pip install langchain ipykernel langchain_community pypdf langchain-text-splitters langchain-openai openai easyocr fitz frontend pymupdf faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install PyMuPDF easyocr pillow langchain"
      ],
      "metadata": {
        "id": "PmWf41kK4U2M"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -y PyMuPDF\n",
        "# !pip install PyMuPDF"
      ],
      "metadata": {
        "id": "YJbrlhLp3llp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip list"
      ],
      "metadata": {
        "id": "9ukXUeAb4SLd"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdf2image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDinxn3o6h0g",
        "outputId": "de5c27c9-6d4e-4195-bcc9-d426b99cba44"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import easyocr\n",
        "import io\n",
        "from PIL import Image\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings  # Updated for embeddings\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain_openai import AzureOpenAIEmbeddings"
      ],
      "metadata": {
        "id": "AL4SVo6M6pD3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract images from PDF\n",
        "def extract_image_from_pdf(pdf_path):\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "    image_paths = []\n",
        "\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        images = page.get_images(full=True)\n",
        "\n",
        "        for image_num, image in enumerate(images):\n",
        "            xref = image[0]\n",
        "            base_image = pdf_document.extract_image(xref)\n",
        "            img_bytes = base_image[\"image\"]\n",
        "            img_pil = Image.open(io.BytesIO(img_bytes))\n",
        "            img_path = f\"/content/drive/MyDrive/output/image_{page_num}_{image_num}.png\"\n",
        "            img_pil.save(img_path)\n",
        "            image_paths.append(img_path)\n",
        "\n",
        "    return image_paths"
      ],
      "metadata": {
        "id": "dL0mU0bk6shg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text from images using EasyOCR\n",
        "def extract_text_from_images_easyocr(image_paths):\n",
        "    reader = easyocr.Reader(['en'])\n",
        "    text = \"\"\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        results = reader.readtext(image_path)\n",
        "        for result in results:\n",
        "            text += result[1] + \"\\n\"\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "cmUsGG3c6yUP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Preprocess the text and create a Document\n",
        "def preprocess_text(text):\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        print(line)"
      ],
      "metadata": {
        "id": "yJFwr_K660zd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text into Langchain's Document structure\n",
        "def load_image_text_as_document(text):\n",
        "    return Document(page_content=text, metadata={\"source\": \"extracted_images\"})"
      ],
      "metadata": {
        "id": "TsbUMaot63Ho"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split documents into chunks\n",
        "def split_docs(data):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    return text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "v_NPNCJV65SX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main flow for processing PDF and images\n",
        "def process_pdf_and_images(pdf_path):\n",
        "    # Extract images and text from the PDF\n",
        "    image_paths = extract_image_from_pdf(pdf_path)\n",
        "    text_from_images = extract_text_from_images_easyocr(image_paths)\n",
        "\n",
        "    # Preprocess and create document for image-based text\n",
        "    preprocess_text(text_from_images)\n",
        "    img_doc = load_image_text_as_document(text_from_images)\n",
        "\n",
        "    # Load the PDF content into Langchain's Document format\n",
        "    pdf_loader = PyPDFLoader(pdf_path)\n",
        "    pdf_docs = pdf_loader.load()\n",
        "\n",
        "    # Append the image-based document to the PDF documents\n",
        "    pdf_docs.append(img_doc)\n",
        "\n",
        "    # Split all documents into smaller chunks\n",
        "    split_documents = split_docs(pdf_docs)\n",
        "    return split_documents"
      ],
      "metadata": {
        "id": "5V9qvAAu67lC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding and Vector Store creation\n",
        "def create_faiss_vector_store(documents):\n",
        "    # Initialize the embedding model\n",
        "    embedding_model =  AzureOpenAIEmbeddings(\n",
        "    azure_endpoint=\"MYENDPOINT\",\n",
        "    api_key=\"MYKEY\",\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    azure_deployment=\"aiml-interns-embedding-ada-002-model\",\n",
        ")\n",
        "\n",
        "    # Create a FAISS vector store from the processed documents\n",
        "    vectorstore = FAISS.from_documents(documents, embedding_model)\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "Rs2uHmRS6-Vz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full execution\n",
        "pdf_path = \"/content/drive/MyDrive/RAG/attention.pdf\"\n",
        "documents = process_pdf_and_images(pdf_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y3w7E9O27B6b",
        "outputId": "235a8920-d274-4387-a5fc-a5db7244e27c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/easyocr/detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
            "/usr/local/lib/python3.10/dist-packages/easyocr/recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output\n",
            "Probabilities\n",
            "Softmax\n",
            "Linear\n",
            "Add & Norm\n",
            "Feed\n",
            "Forward\n",
            "Add & Norm\n",
            "Add & Norm\n",
            "Multi-Head\n",
            "Feed\n",
            "Attention\n",
            "Forward\n",
            "Nx\n",
            "Add & Norm\n",
            "Nx\n",
            "Add & Norm\n",
            "Masked\n",
            "Multi-Head\n",
            "Multi-Head\n",
            "Attention\n",
            "Attention\n",
            "Positional\n",
            "Positional\n",
            "Encoding\n",
            "Encoding\n",
            "Input\n",
            "Output\n",
            "Embedding\n",
            "Embedding\n",
            "Inputs\n",
            "Outputs\n",
            "(shifted right)\n",
            "Linear\n",
            "Concat\n",
            "Scaled Dot-Product\n",
            "Attention\n",
            "Linear\n",
            "Linear\n",
            "Linear\n",
            "K\n",
            "MatMul\n",
            "SoftMax\n",
            "Mask (opt )\n",
            "Scale\n",
            "MatMul\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = create_faiss_vector_store(documents)"
      ],
      "metadata": {
        "id": "ErjKsCwW7l-g"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "import os\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"MYKEY\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"MYENDPOINT\""
      ],
      "metadata": {
        "id": "o5UTJK708DLB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "JounjO4W8IMP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import  FAISS\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "template = \"\"\"You are an AI assistant for question-answering tasks.\n",
        "        Use the following pieces of context to answer the question. If you don't know the\n",
        "        answer, please output 'No answer'.Use Maximum ten sentence and keep the answer concise\n",
        "        Question: {question} Context: {context}\n",
        "        \"\"\""
      ],
      "metadata": {
        "id": "HdU23G4K8Krq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.schema.runnable import  RunnablePassthrough\n",
        "from langchain.schema.output_parser import  StrOutputParser"
      ],
      "metadata": {
        "id": "OBj17TIL8MlA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "n7nSipmv8PE3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=\"MYENDPOINT\",\n",
        "    api_key=\"MYKEY\",\n",
        "    model = \"gpt-35-turbo\",\n",
        "    max_tokens=50,\n",
        "    azure_deployment=\"GPT-35-TURBO\",\n",
        "    api_version=\"2024-06-01\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA74xcp08Rlr",
        "outputId": "a9a9e8e3-46f9-4f14-d0f5-6e79cf3f5003"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-263f8acba241>:1: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
            "  llm = AzureChatOpenAI(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "EruOV8sW8Ttp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "# Create a question-answering chain (QA Chain) that will use the LLM\n",
        "qa_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)\n",
        "\n",
        "# Define the ConversationalRetrievalChain to structure the pipeline\n",
        "chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2qcOGeV8WRv",
        "outputId": "a471eb24-f8c7-43d2-9e37-0be11a52da1e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-8f3fdf5ae8f3>:4: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
            "stuff: https://python.langchain.com/v0.2/docs/versions/migrating_chains/stuff_docs_chain\n",
            "map_reduce: https://python.langchain.com/v0.2/docs/versions/migrating_chains/map_reduce_chain\n",
            "refine: https://python.langchain.com/v0.2/docs/versions/migrating_chains/refine_chain\n",
            "map_rerank: https://python.langchain.com/v0.2/docs/versions/migrating_chains/map_rerank_docs_chain\n",
            "\n",
            "See also guides on retrieval and question-answering here: https://python.langchain.com/v0.2/docs/how_to/#qa-with-rag\n",
            "  qa_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example question\n",
        "question = \"What is the BLEU Score of EN-DE of model ConvS2S Ensemble?\"\n",
        "\n",
        "# Initialize an empty chat history\n",
        "chat_history = []\n",
        "\n",
        "# Invoke the chain\n",
        "try:\n",
        "    response = chain({\"question\": question, \"chat_history\": chat_history})\n",
        "    answer = response[\"answer\"]\n",
        "    source_documents = response[\"source_documents\"]\n",
        "\n",
        "    print(\"Answer:\", answer)\n",
        "    print(\"Source Documents:\", source_documents)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q26ElGPO8ZYA",
        "outputId": "bbb6566e-de19-4171-e3cc-98823bf66a4d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-869585f555d2>:9: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
            "  response = chain({\"question\": question, \"chat_history\": chat_history})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The BLEU score of the EN-DE model ConvS2S Ensemble is 26.36.\n",
            "Source Documents: [Document(metadata={'source': '/content/drive/MyDrive/RAG/attention.pdf', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0·1020\\nGNMT + RL [31] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [8] 25.16 40.46 9.6·10181.5·1020\\nMoE [26] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.0 2.3·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)'), Document(metadata={'source': '/content/drive/MyDrive/RAG/attention.pdf', 'page': 0}, page_content='be superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].'), Document(metadata={'source': '/content/drive/MyDrive/RAG/attention.pdf', 'page': 7}, page_content='On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which'), Document(metadata={'source': '/content/drive/MyDrive/RAG/attention.pdf', 'page': 7}, page_content='To evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the BLEU Score of EN-FR of model Transformer(big)?\"\n",
        "\n",
        "# Initialize an empty chat history\n",
        "chat_history = []\n",
        "\n",
        "# Invoke the chain\n",
        "try:\n",
        "    response = chain({\"question\": question, \"chat_history\": chat_history})\n",
        "    answer = response[\"answer\"]\n",
        "    source_documents = response[\"source_documents\"]\n",
        "\n",
        "    print(\"Answer:\", answer)\n",
        "    print(\"Source Documents:\", source_documents)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0fTb4Nz8h2p",
        "outputId": "7a0e8158-2b7c-4bcf-debd-34b087565755"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The BLEU score of the EN-FR translation task for the Transformer (big) model is 41.0.\n",
            "Source Documents: [Document(metadata={'source': '/content/drive/MyDrive/RAG/attention.pdf', 'page': 7}, page_content='On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which'), Document(metadata={'source': '/content/drive/MyDrive/RAG/attention.pdf', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0·1020\\nGNMT + RL [31] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [8] 25.16 40.46 9.6·10181.5·1020\\nMoE [26] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.0 2.3·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)'), Document(metadata={'source': '/content/drive/MyDrive/RAG/attention.pdf', 'page': 0}, page_content='be superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].'), Document(metadata={'source': '/content/drive/MyDrive/RAG/attention.pdf', 'page': 7}, page_content='To evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XsF8wIfF9NyT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}